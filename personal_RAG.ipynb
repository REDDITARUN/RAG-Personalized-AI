{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: langchain in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (0.3.3)\n","Requirement already satisfied: langchain-community in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (0.3.2)\n","Requirement already satisfied: pypdf in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (5.0.1)\n","Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.35)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (3.10.9)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.10)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.1.132)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\python312\\lib\\site-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.5.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.14.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n","Requirement already satisfied: httpcore==1.* in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n","Requirement already satisfied: sniffio in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n"]}],"source":["!pip install langchain langchain-community pypdf"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.9.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.1)\n","Downloading faiss_cpu-1.9.0-cp312-cp312-win_amd64.whl (14.9 MB)\n","   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n","   --- ------------------------------------ 1.3/14.9 MB 9.6 MB/s eta 0:00:02\n","   ------------------- -------------------- 7.3/14.9 MB 22.7 MB/s eta 0:00:01\n","   --------------------------- ------------ 10.2/14.9 MB 23.6 MB/s eta 0:00:01\n","   ---------------------------------------- 14.9/14.9 MB 21.3 MB/s eta 0:00:00\n","Installing collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.9.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install faiss-cpu\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n","\n","Collecting langchain-ollama\n","  Downloading langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-ollama) (0.3.10)\n","Collecting ollama<1,>=0.3.0 (from langchain-ollama)\n","  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.1.132)\n","Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (24.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.9.2)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (8.5.0)\n","Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (4.12.2)\n","Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from ollama<1,>=0.3.0->langchain-ollama) (0.27.2)\n","Requirement already satisfied: anyio in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (4.6.0)\n","Requirement already satisfied: certifi in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.6)\n","Requirement already satisfied: idna in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.10)\n","Requirement already satisfied: sniffio in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.10.7)\n","Requirement already satisfied: requests<3,>=2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiyo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.2.3)\n","Downloading langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n","Downloading ollama-0.3.3-py3-none-any.whl (10 kB)\n","Installing collected packages: ollama, langchain-ollama\n","Successfully installed langchain-ollama-0.2.0 ollama-0.3.3\n"]}],"source":["pip install langchain-ollama\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from langchain_community.embeddings import OllamaEmbeddings\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import OllamaEmbeddings\n","from langchain_ollama import ChatOllama\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.prompts import PromptTemplate\n","from operator import itemgetter"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["PDF_FILE = \"PEFT_LORA.pdf\" # leme load the novel first\n","MODEL = \"llama3.2:1b\" # on the next step we we will load the model"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of pages: 28\n","Length of a page: 879\n","Content of a page: PEFT\n","2This keeps the number of trainable p arameters low without sacr ificing \n","performance.\n","Quick Mat h R ec ap:\n","W  W ₀ + ΔW\n","ΔW  BA\n","W is the updat ed weight matr ix, and W ₀ is the or iginal one. B and A ar e the \n","low-rank matr ices that r epresent the updat e.\n","In simple t erms, instead of updating all the p arameters of a w eight matr ix, LoRA  \n","uses smaller matr ices to make targeted adjustment s, reducing the comput ation \n","load.\n","import numpy as np\n","# Original weight matrix\n","W0 = np.array([[1, 2, 3],\n","                [4, 5, 6]])\n","# Low-rank matrices\n","A = np.array([[0.1, 0.2]])  # Shape : (1, 2)\n","B = np.array([[0.3],\n","              [0.4],\n","              [0.5]])  # Shape : (3, 1)\n","# LoRA update\n","delta_W = np.dot(A.T, B.T)\n","# Final adapted weight\n","W = W0 + delta_W\n","print(\"Original W0:\" )\n","print(W0)\n","print(\"\\nLoRA update (ΔW):\" )\n","print(delta_W)\n","print(\"\\nFinal adapted W:\" )\n","print(W)\n"]}],"source":["\n","loader = PyPDFLoader(PDF_FILE)\n","pages = loader.load()\n","\n","print(f\"Number of pages: {len(pages)}\")\n","print(f\"Length of a page: {len(pages[1].page_content)}\")\n","print(\"Content of a page:\", pages[1].page_content)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of chunks: 28\n","Length of a chunk: 879\n","Content of a chunk: PEFT\n","2This keeps the number of trainable p arameters low without sacr ificing \n","performance.\n","Quick Mat h R ec ap:\n","W  W ₀ + ΔW\n","ΔW  BA\n","W is the updat ed weight matr ix, and W ₀ is the or iginal one. B and A ar e the \n","low-rank matr ices that r epresent the updat e.\n","In simple t erms, instead of updating all the p arameters of a w eight matr ix, LoRA  \n","uses smaller matr ices to make targeted adjustment s, reducing the comput ation \n","load.\n","import numpy as np\n","# Original weight matrix\n","W0 = np.array([[1, 2, 3],\n","                [4, 5, 6]])\n","# Low-rank matrices\n","A = np.array([[0.1, 0.2]])  # Shape : (1, 2)\n","B = np.array([[0.3],\n","              [0.4],\n","              [0.5]])  # Shape : (3, 1)\n","# LoRA update\n","delta_W = np.dot(A.T, B.T)\n","# Final adapted weight\n","W = W0 + delta_W\n","print(\"Original W0:\" )\n","print(W0)\n","print(\"\\nLoRA update (ΔW):\" )\n","print(delta_W)\n","print(\"\\nFinal adapted W:\" )\n","print(W)\n"]}],"source":["splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=150)\n","\n","chunks = splitter.split_documents(pages)\n","print(f\"Number of chunks: {len(chunks)}\")\n","print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n","print(\"Content of a chunk:\", chunks[1].page_content)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Kiyo\\AppData\\Local\\Temp\\ipykernel_20212\\3103334380.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n","  embeddings = OllamaEmbeddings(model=MODEL)\n"]}],"source":["\n","\n","embeddings = OllamaEmbeddings(model=MODEL)\n","vectorstore = FAISS.from_documents(chunks, embeddings) # storein vector store"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 5}, page_content='PEFT\\n6import numpy as np\\n# Original activations\\nx = np.array([[1, 2, 3],\\n              [4, 5, 6]])\\n# Learned rescaling vector\\nl = np.array([0.5, 1.5, 1.0])\\n# IA³ rescaling\\ny = x * l  # Element -wise multiplication\\nprint(\"Original activations:\" )\\nprint(x)\\nprint(\"\\\\nRescaling vector:\" )\\nprint(l)\\nprint(\"\\\\nRescaled activations:\" )\\nprint(y)\\nOriginal activations :\\n[[1 2 3]\\n [4 5 6]]\\nRescaling vector :\\n[0.5 1.5 1. ]\\nRescaled activations :\\n[[0.5 3.  3. ]\\n [2.  7.5 6. ]]\\nWhat ʼ s happening?\\nWe have some activ ations \\nx that we want t o modify . Instead of changing them all equall y, we use a \\nlearned vector l to scale them dif ferently. This wa y, some p arts of the model  \\nget more or less at tention, which helps it adapt bet ter to the task at hand.\\n4 .  L o H a  ( L o w - R a n k  H a d a m a r d  P r o d u c t ) :'),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 4}, page_content=\"PEFT\\n5AdaLoRA update (ΔW):\\n[[0.084 0.095 0.106]\\n [0.198 0.225 0.252]]\\nFinal adapted W:\\n[[1.084 2.095 3.106]\\n [4.198 5.225 6.252]]\\nPruned AdaLoRA update (ΔW):\\n[[0.03  0.035 0.04 ]\\n [0.09  0.105 0.12 ]]\\nWhat ʼ s happening?\\nHere, AdaLoRA uses something c alled SVD \\ue081Singular V alue Decomposition). W e \\ntake \\nP, Lambda, and Q (all parts of the SVD\\ue082 t o create an updat e ΔW that adapt s the \\nweight matr ix. Then, w e can adjust Lambda (pruning less impor tant values) to \\nsee how it impacts training. I tʼs a way to fine-tune onl y the crucial p arts of the \\nmodel without wasting r esources.\\n3 .  I A ³  ( I n f u s e d  A d a p t e r  b y  I n h i b i t i n g  a n d  A m p l i f y i n g  I n n e r  \\nA c t i v a t i o n s ) :\\nIA³ uses le arned vectors to scale specific model activ ations, making it a  \\nversatile and ef ficient method.\\nHo w it W or ks:\\nIntroduces le arned vectors to modify activ ations in self -attention and f eed-\\nforward layers through element -wise mul tiplication.\\nThis lets the model amplify or suppr ess different activ ation dimensions  \\ndepending on the t ask.\\nK e y F or mula:\\ny = l ⊙  x\\nl is a learned vector, x is the activ ations sequence, and ⊙  represents \\nelement -wise mul tiplication.\\nIA³ tweaks the model's inner w orkings to control which p arts are more or less  \\nimportant, making it gr eat for mixed-task models.\"),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 11}, page_content='PEFT\\n12In this tut orial, weʼll be using Vision T ransformer as our b ase model with t wo \\ndatasets. The first one, Human Action R ecognition, f eatures 15 dif ferent classes  \\nof human activities, t otaling over 12,000 labeled images. Each image r epresents \\na single activit y, saved in sep arate folders accor ding to its class.\\nThe second dat aset is the UC Mer ced Land Use dat aset, which includes 21  \\ndifferent classes of land use images, with 100 images per class, co vering \\ncategories like agricultural fields, b aseball diamonds, be aches, and mor e. Each \\nimage is 256\\ue09f256 pix els, extracted from high-r esolution USGS National Map  \\nUrban Area Imager y.\\nNow that youʼre all caught up, letʼ s get int o the real action and e xplore how to \\nmake these adapt ers work their magic! F or the complet e code, he ad over to the \\nGitHub link belo w, or check out m y Bento website to connect and f ollow me on  \\nother plat forms.\\nThis code is p acked with a v ariety of helper functions and logic aimed at fine-\\ntuning and e valuating image classific ation models using LoRA \\ue081Lo w-Rank \\nAdaptation) and other similar adapt ers.\\nHelper F unctions:\\nThe first f ew helpers ar e quite useful, st arting with print_model_size(). It lets you \\nknow just ho w much y our model is hogging up sp ace. Then, w e have \\nprint_trainable_parameters(), which giv es you a reality check on ho w much of the  \\nmodel youʼre actuall y training.\\n# Helper functions\\ndef print_model_size(path):\\n    size = sum(os.path.getsize(f) for f in os.scandir(path))\\n    print(f\"Model size: {(size / 1e6):.2f} MB\")\\ndef print_trainable_parameters(model, label):\\n    parameters, trainable = 0, 0\\n    for _, p in model.named_parameters():\\n        parameters += p.numel()\\n        trainable += p.numel() if p.requires_grad else 0\\n    print(f\"{label} trainable parameters: {trainable:,}/{param\\nSplit ting t he Dat aset:'),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 22}, page_content=\"PEFT\\n23\\n\\ue072\\ue094\\x00 LoRA A dapt er:\\nAccur ac y: 0.71\\nF1 Scor e: 0.69\\nLoRA managed t o inch up t o decent accurac y, but itʼs clear that itʼ s \\nmore suited for classific ation tasks than the mul ti-task comple xity of \\nDataset 1. The model did bet ter after a few epochs, but still f ell short in \\nhandling nuanced distinctions.\\n\\ue073\\ue094\\x00 A daLoRA A dapt er:\\nAccur ac y: 0.54\\nF1 Scor e: 0.46\\nThis one had a shaky st art and just couldn ʼt pick up p ace. AdaLoRA,  \\ndesigned f or adapt ability, didn't adapt so w ell here. It struggled with  \\nboth precision and r ecall, indic ating it wasn ʼt keeping up with the  \\nmodelʼs learning demands.\")]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["retriever = vectorstore.as_retriever()\n","retriever.invoke(\"What is LORA?\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 5}, page_content='PEFT\\n6import numpy as np\\n# Original activations\\nx = np.array([[1, 2, 3],\\n              [4, 5, 6]])\\n# Learned rescaling vector\\nl = np.array([0.5, 1.5, 1.0])\\n# IA³ rescaling\\ny = x * l  # Element -wise multiplication\\nprint(\"Original activations:\" )\\nprint(x)\\nprint(\"\\\\nRescaling vector:\" )\\nprint(l)\\nprint(\"\\\\nRescaled activations:\" )\\nprint(y)\\nOriginal activations :\\n[[1 2 3]\\n [4 5 6]]\\nRescaling vector :\\n[0.5 1.5 1. ]\\nRescaled activations :\\n[[0.5 3.  3. ]\\n [2.  7.5 6. ]]\\nWhat ʼ s happening?\\nWe have some activ ations \\nx that we want t o modify . Instead of changing them all equall y, we use a \\nlearned vector l to scale them dif ferently. This wa y, some p arts of the model  \\nget more or less at tention, which helps it adapt bet ter to the task at hand.\\n4 .  L o H a  ( L o w - R a n k  H a d a m a r d  P r o d u c t ) :'),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 4}, page_content=\"PEFT\\n5AdaLoRA update (ΔW):\\n[[0.084 0.095 0.106]\\n [0.198 0.225 0.252]]\\nFinal adapted W:\\n[[1.084 2.095 3.106]\\n [4.198 5.225 6.252]]\\nPruned AdaLoRA update (ΔW):\\n[[0.03  0.035 0.04 ]\\n [0.09  0.105 0.12 ]]\\nWhat ʼ s happening?\\nHere, AdaLoRA uses something c alled SVD \\ue081Singular V alue Decomposition). W e \\ntake \\nP, Lambda, and Q (all parts of the SVD\\ue082 t o create an updat e ΔW that adapt s the \\nweight matr ix. Then, w e can adjust Lambda (pruning less impor tant values) to \\nsee how it impacts training. I tʼs a way to fine-tune onl y the crucial p arts of the \\nmodel without wasting r esources.\\n3 .  I A ³  ( I n f u s e d  A d a p t e r  b y  I n h i b i t i n g  a n d  A m p l i f y i n g  I n n e r  \\nA c t i v a t i o n s ) :\\nIA³ uses le arned vectors to scale specific model activ ations, making it a  \\nversatile and ef ficient method.\\nHo w it W or ks:\\nIntroduces le arned vectors to modify activ ations in self -attention and f eed-\\nforward layers through element -wise mul tiplication.\\nThis lets the model amplify or suppr ess different activ ation dimensions  \\ndepending on the t ask.\\nK e y F or mula:\\ny = l ⊙  x\\nl is a learned vector, x is the activ ations sequence, and ⊙  represents \\nelement -wise mul tiplication.\\nIA³ tweaks the model's inner w orkings to control which p arts are more or less  \\nimportant, making it gr eat for mixed-task models.\"),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 11}, page_content='PEFT\\n12In this tut orial, weʼll be using Vision T ransformer as our b ase model with t wo \\ndatasets. The first one, Human Action R ecognition, f eatures 15 dif ferent classes  \\nof human activities, t otaling over 12,000 labeled images. Each image r epresents \\na single activit y, saved in sep arate folders accor ding to its class.\\nThe second dat aset is the UC Mer ced Land Use dat aset, which includes 21  \\ndifferent classes of land use images, with 100 images per class, co vering \\ncategories like agricultural fields, b aseball diamonds, be aches, and mor e. Each \\nimage is 256\\ue09f256 pix els, extracted from high-r esolution USGS National Map  \\nUrban Area Imager y.\\nNow that youʼre all caught up, letʼ s get int o the real action and e xplore how to \\nmake these adapt ers work their magic! F or the complet e code, he ad over to the \\nGitHub link belo w, or check out m y Bento website to connect and f ollow me on  \\nother plat forms.\\nThis code is p acked with a v ariety of helper functions and logic aimed at fine-\\ntuning and e valuating image classific ation models using LoRA \\ue081Lo w-Rank \\nAdaptation) and other similar adapt ers.\\nHelper F unctions:\\nThe first f ew helpers ar e quite useful, st arting with print_model_size(). It lets you \\nknow just ho w much y our model is hogging up sp ace. Then, w e have \\nprint_trainable_parameters(), which giv es you a reality check on ho w much of the  \\nmodel youʼre actuall y training.\\n# Helper functions\\ndef print_model_size(path):\\n    size = sum(os.path.getsize(f) for f in os.scandir(path))\\n    print(f\"Model size: {(size / 1e6):.2f} MB\")\\ndef print_trainable_parameters(model, label):\\n    parameters, trainable = 0, 0\\n    for _, p in model.named_parameters():\\n        parameters += p.numel()\\n        trainable += p.numel() if p.requires_grad else 0\\n    print(f\"{label} trainable parameters: {trainable:,}/{param\\nSplit ting t he Dat aset:'),\n"," Document(metadata={'source': 'PEFT_LORA.pdf', 'page': 21}, page_content=\"PEFT\\n22            # Check for mismatch and align lengths if necessa\\n            if len(epochs) != len(metric_values ):\\n                min_length = min(len(epochs), len(metric_value\\n                epochs = epochs[:min_length ]\\n                metric_values = metric_values [:min_length ]\\n            # Sort the data points by epoch\\n            sort_indices = np.argsort(epochs)\\n            epochs = epochs[sort_indices ]\\n            metric_values = metric_values [sort_indices ]\\n            ax .plot(epochs, metric_values , label=adapter, mar\\n        ax .set_title (metric.capitalize ())\\n        ax .set_xlabel ('Epoch')\\n        ax .set_ylabel (metric.capitalize ())\\n        ax .legend()\\n    plt.tight_layout ()\\n    plt.savefig(f'{model_name}_metrics_comparison.png' )\\n    plt.show()\\n    plt.close()\\nT i m e  f o r  R e s u l t s\\nAlright, letʼs dive into this models sho wdown between Dataset 1 and Dat aset 2, \\neach armed with dif ferent adapt ers. \\nM o d e l  1 ʼ s  o n  D a t a s e t  1  -  T h e  S t r u g g l e  B u s\\nThis model tr ied its best with e ach adapt er, but it faced some ser ious \\nchallenges. Her eʼs the quick rundo wn:\")]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["retriever = vectorstore.as_retriever()\n","retriever.invoke(\"Explain LORA types?\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["AIMessage(content='I can provide information on various types of Low-Rate Audio (LORA) systems. Here are some examples:\\n\\n1. **LORAN C**: A navigation system that uses a network of radio transmitters and receivers to provide location information.\\n2. **LORAN A**: An improved version of the LORAN C system, which provides more accurate location information.\\n3. **LORAN V**: The latest generation of the LORAN system, which offers even better accuracy and reliability.\\n4. **LORAN C1**: A variant of the LORAN C system that uses a single transmitter to provide location information.\\n5. **LORAN C2**: An improved version of the LORAN C system that provides more accurate location information.\\n\\nIn terms of specific applications, LORA systems are often used in:\\n\\n1. **Navigation and mapping**: LORAN is commonly used in navigation systems such as GPS (Global Positioning System) and GLONASS.\\n2. **Emergency services**: LORAN is used by emergency services such as ambulance and fire departments to provide location information during emergencies.\\n3. **Agriculture**: LORAN is used in agriculture to track the movement of livestock and crops.\\n4. **Surveying and mapping**: LORAN is used in surveying and mapping applications, such as creating topographic maps.\\n\\nThese are just a few examples of LORA types and their uses. If you have any specific questions or would like more information on a particular type of LORA system, feel free to ask!', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2024-10-10T01:21:06.6846645Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 32839515800, 'load_duration': 85357500, 'prompt_eval_count': 32, 'prompt_eval_duration': 1051185000, 'eval_count': 316, 'eval_duration': 31696392000}, id='run-a3a52fb4-87e2-4b6a-b898-ddf5b82a1576-0', usage_metadata={'input_tokens': 32, 'output_tokens': 316, 'total_tokens': 348})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model = ChatOllama(model=MODEL, temperature=0)\n","model.invoke(\"What LORA types you know?\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["I can provide information on various types of Low-Rate Area Network (LRAN) technologies. Here are some of the most well-known ones:\n","\n","1. **LORAN-C**: A satellite-based navigation system that uses a network of satellites to provide location information and timing signals.\n","2. **LORAN-A**: An older version of LORAN, which was used until 2006. It's still in use today, but it has been largely replaced by more modern systems.\n","3. **LORAN-C/CRS (Cesium-Radio Star)**: A variant of LORAN that uses cesium atomic clocks to provide more accurate location information.\n","4. **LORAN-C/CRS-2**: An updated version of the CRS system, which offers improved accuracy and reliability.\n","5. **LORAN-C/CRS-3**: The latest version of the CRS system, which provides even higher accuracy and reliability than its predecessors.\n","\n","These are some of the most common types of Loran systems used in navigation applications. There may be other variants or specialized systems as well.\n"]}],"source":["\n","\n","parser = StrOutputParser()\n","\n","chain = model | parser \n","print(chain.invoke(\"What LORA types you know?\"))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","You are an assistant that provides answers to questions based on\n","a given information. \n","\n","You need to answer the question based on the given information If you can't answer the\n","question, reply \"I don't know pallll\".\n","\n","Be as concise as possible and go straight to the point.\n","\n","Context: Here is some context\n","\n","Question: Here is a question\n","\n"]}],"source":["\n","\n","template = \"\"\"\n","You are an assistant that provides answers to questions based on\n","a given information. \n","\n","You need to answer the question based on the given information If you can't answer the\n","question, reply \"I don't know pallll\".\n","\n","Be as concise as possible and go straight to the point.\n","\n","Context: {context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = PromptTemplate.from_template(template)\n","print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["'LoRaWAN is a wireless communication technology used for IoT (Internet of Things) applications. There are three main LoRaWAN types:\\n\\n1. **LoRaWAN Classic**: The original version, using 868 MHz frequency band.\\n2. **LoRaWAN Cat B20**: Using 868 MHz and 915 MHz frequencies, offering better range and performance.\\n3. **LoRaWAN Cat NB**: Using 868 MHz and 915 MHz frequencies, optimized for low-power applications.\\n\\nEach type has its own characteristics, advantages, and use cases.'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["chain = prompt | model | parser\n","\n","chain.invoke({\n","    \"context\": \"LoRA\", \n","    \"question\": \"Explain LoRA types?\"\n","})\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","\n","chain = (\n","    {\n","        \"context\": itemgetter(\"question\") | retriever,\n","        \"question\": itemgetter(\"question\"),\n","    }\n","    | prompt\n","    | model\n","    | parser\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: Give information on Parameter-Efficient Fine-Tuning\n","Answer: Parameter-Efficient Fine-Tuning (PEFT) is a technique used in deep learning models to fine-tune them on specific tasks or datasets without increasing their computational requirements. Here are some key points about PEFT:\n","\n","1. **Principle**: PEFT involves using learned vectors, called \"IA³\" (Inferred Activation 3), to scale model activations and adapt the weights of a pre-trained model.\n","2. **Advantages**:\n","\t* Reduces computational requirements by only scaling specific parts of the model.\n","\t* Can be used for mixed-task models where different tasks require different adaptations.\n","\t* Allows for more efficient training on smaller datasets or with lower computational resources.\n","3. **Components**: PEFT typically involves three main components:\n","\t* IA³: A learned vector that scales activations in self-attention and feed-forward layers.\n","\t* Learned vectors (L): Pre-trained weights of the model used to scale activations.\n","4. **Training process**:\n","\t* The pre-trained model is fine-tuned on a specific task or dataset using the IA³ learned vectors as adaptors.\n","\t* The goal is to adjust the weights of the pre-trained model while keeping its pre-trained parameters fixed.\n","\n","By leveraging IA³ and learned vectors, PEFT can significantly reduce computational requirements without sacrificing performance.\n","*************************\n","\n"]}],"source":["questions = [\n","    \"Give information on Parameter-Efficient Fine-Tuning\",\n","\n","]\n","\n","for question in questions:\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {chain.invoke({'question': question})}\")\n","    print(\"*************************\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5848562,"sourceId":9589556,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":4}
